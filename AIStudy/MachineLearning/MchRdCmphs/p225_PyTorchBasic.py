#深度学习框架PyTorch基础   P244

'''
机器学习基础 P205
按照训练数据中有无输出数据的标注分为监督学习(supervised learning)、无监督学习和半监督学习(semi-supervised learning)
监督学习的训练数据中包括输入范例和对应的输出范例 利用观察和统计输入输出之间的关系训练模型 并通过比对训练结果和标注结果之间的差异调整模型参数 提高预测性能 效果一般较好 但非常依赖标注数据的大小 亦花费大量人力和时间成本
无监督学习指训练数据没有对输出进行标注 需要模型自行定义训练标准 以达预期目标 如聚类 难度远大于监督学习 效果一般也逊于后者 可以有效利用海量未标注数据 特别适合NLP相关应用
半监督学习处理上述两者之间 一种方法是未标注数据进行预训练 通过少量标注数据引导模型进一步提高准确度

模型 建立模型是为了找到自变量房子的d种信息x=(x1,x2,...,xd)与因变量y价格的关系
参数 可能的关系有无数种 通过参数β0,β1...,βd来限定范围 y = β0 + β1x1 + β2x2 +...+ βdxd
     二次模型：y=β(0,0)+∑(i,∑(j,β(1,j)xixj))
不同模型的假设范围大小不一 参数个数和复杂程度也不一样 需要将参数调整到合适的值 训练模型的过程是根据输入和输出不断改变模型参数的取值 以得到具有一定准确度的模型 即参数优化(parameter optimization)

训练集 (training set)模型用于训练的数据
测试集 (test set)推广 泛化 在训练集上检验模型性能无法充分区分模型优劣 因此需要同分布却未出现的数据担当测试集 即关心模型在未见过数据上的推广能力 亦称泛化能力
过拟合 (overfitting) 模型本应在训练集上不断提升准确度 却在超过一定的阈值后于训练集上表现越好的模型在测试集上表现越差
验证集 (validation set) 为在训练中观察到过拟合现象 将训练集中分出一部分数据作为验证集 验证集准确度有下滑即可中止优化 选出之前验证集表现最好的模型作结果
'''


'''
深度学习基础 P208
深度学习 最重要的基础组成部分是神经网络
神经网络 指按照人类脑部神经元工作原理进行抽象而建立的一种人工智能运算模型 基本组成单元是人工神经元(neuron) 可输入处理及输出信息 由有权重的边相互连接并进行信息传递 大量神经元组成的复杂网络即神经网络
神经元 亦称感知元(perceptron) 神经网络的最小组成单元 包含n条输入信息x=(x1,x2,...,xn)和1条输出信息 信息均为实数 n条边的权重参数w=(w1,w2,...,wn)
     神经元进行加权和处理 U(x;w)=w1x1+w2x2+...+wnxn=∑(n,i+1,wixi)=w^Tx
     为输出更加灵活一般给结果加上截距 S(x;w,b)=U(x,w)+b=W^T+b
     即S(x;w,b)是关于输入(x1,x2,...,xn)的线性函数
     通过激活函数(activation function)g对加权和进行变换 得到最终输出f(x;w,b)=g(S(x;w,b))
激活函数 性质:1)是一个连续可导的一元非线性函数 允许在常数个点上不可导 目的是便于已有的数值优化工具对其进行求导并优化
             2)激活函数和其导数的计算须较为简单 以提高计算效率 因网络中大量参与运算和求导
             3)因神经网络求导中运用到乘积形式的链式法则 激活函数的导数值域需在一个合理的区间中 以防链式法则中导数的绝对值过大或过小
         常见形式:1)sigmoid函数 σ(x)=1/(1+e^-x) S形曲线 x=0时取值0.5 x→+∞时趋向1 x→-∞时趋向0 收敛速度很快 导数σ'(x)=σ(x)(1-σ(x)) 导数值域(0,0.25] 连续可导 运算简单 函数值域和导数值域均为限定区间 最常用的激活函数之一
                 2)tanh函数 双正切函数 tanh(x)=(e^x-e^-x)/(e^x+e^-x) S行曲线 值域(-1,1) x=0时取值0 x→+∞时趋向1 x→-∞时趋向-1 收敛速度很快 导数tanh'(x)=1-tanh^2(x) 导数值域(0,1] 原点对称 零中心化(zero-centered)
                 3)ReLU函数 修正线性单元(Rectified Linear Unit) 小于0时输出0 大于等于0时直接输出ReLU(x)={x,x>=0;0,x<0}=max(x,0) 导数远离零点 可使优化算法对参数进行有效调控 缺点当参数小于0导数永远是0优化算法无法改变参数的值

前馈神经网络 (feedforward neural network) n个神经元共享输入 并经过计算得到n个输出 形成一个层(level) 作为下一层神经元的输入 层与层通过有权重的边进行连接 形成基本的神经网络形式
     组成 第一层:输入层(Input Level) 非神经元 n个输入值 整个网络的外部输入
          最后一层:输出层(Output Level) 输出作为网络的输出
          两者间层:隐藏层(Hidden Level) 之间的层的统称
     所有相邻层之间均由带权边连接 每层神经元个数不必一致 有各自的截距和参数 充分灵活
     计算过程被称作向前传播(forward pass)
     
损失函数 (loss function)训练神经网络的目的在于使其输出符合预期真值(ground truth) 但其参数在初始化时是随机分布的 需要调整网络参数以提高模型表现 一种方法是通过输出和预期真值的差值求导来得到调整参数的方向 但该差值往往不是一个可导数如分类任务中的正确率 于是多由损失函数估算 损失函数是神经网络中所有参数的函数 它刻画了网络输出与预期真值之间的差异 对同一问题损失函数定义不唯一 但需三点性质：
     1.需要关于神经网络中的所有参数可导 2.定义是函数值越小模型准确度越高 3.需要近似表示结果的不准确性
          近似表示:真正的评测标准下对于一个输入网络参数产生结果好于另组网络参数及结果 无需保证损失函数结果最优 但需要大多情况下满足
     常见类型:均方差损失函数 均方差(Mean Squared Error,MSE) “差的平方的平方”的缩写 若预测值与预期真值接近 损失函数值以平方级缩小 适用于回归类(regression)问题
             交叉熵损失函数 如分类任务 取最大分数下标对于网络参数不可导 通过softmax操作将分数变为概率值 交叉熵希望最大化对应类的概率 即最大化可能性的对数 即最小化预测正确类别的概率的对数的相反数 因概率值范围0~1 交叉熵一定是非负数

梯度下降 (gradient descent)神经网络中最常用的优化方法 一种系统的优化方法来更新初始随机的参数 以达到损失函数值不断减小的目的
     梯度 (gradient)对于有多个参数的函数 可求出函数对于每一个参数的导数 这些导数形成的向量称为梯度
          如f(x)=f(x1,x2,...,xn) 每参数导数∂f/∂xi 导数形成的向量∇f(x)|x=(x1,x2,...,xn)=[∂f/∂x1,∂f/∂x2,...,∂f/∂xn]为梯度
     学习率 (learning rate)所有的参数沿着其梯度的反方向移动 距离用学习率α控制 即xi←xi-α(∂f/∂xi)
反向传播 (back propagation)梯度下降法为求出函数当前参数点梯度 即损失函数关于神经网络中所有参数的导数 神经网络结构复杂难以得到每个参数导数公式 遂采用反向传播 利用了微积分中导数计算的链式法则(chain rule)
     链式法则 如果f(g(x))是一个复合函数 即f是g的函数 而g是x的函数 那么f关于x的导数为df(g(x))/dx=(df(g)/dg)(dg(x)/dx)
       例如f(g)=g^2,g(x)=x^3 那么f(g(x))=x^6 所以df(g(x))/dx=6x^5 根据链式法则df(g(x))/dx=(df(g)/dg)(dg(x)/dx)=2g×3x^2=6x^5
       链式法则可以用在神经网络对参数求导的过程中 可得到损失函数关于最后一层边权的导数 继续计算关于前一层边权导数 直到得出全部导数 按学习率沿导数反向移动 以使损失值不断下降
     梯度爆炸 当网络层数变多 导数经过链式法则的乘法操作 很容易在靠近输入层的部分发生导数爆炸和导数消失的情况 多见于多个绝对值大于1的导数或多个绝对值小于1的导数相乘

深度学习中常见神经网络类型 P218
===========TODO:继续啃这里============
前馈神经网络 (FNN)上文
卷积神经网络 (CNN)也被称为全连接网络(fully connected network)
循环神经网络 (RNN)
丢弃 (Dropout)
(没有提及对抗生成网络？)

深度学习框架PyTorch P224 如下
'''

print("=====初始化张量代码示例=====")

import torch
#一个大小为2 x 3的实数型张量
a = torch.FloatTensor([[1.2, 3.4, 5], [3, 6, 7.4]])
print(a)
#一个大小为5 x 6的实数型张量，每个元素根据标准正态分布N(0,1)随机采样
b = torch.randn(5, 6) 
print(b)
#将a第0行第2列的元素改为4.0，变为([[1.2, 3.4, 4.0], [3, 6, 7.4]])
a[0, 2] = 4.0 
print(a)

print("=====运算和求导示例=====")

import torch
a = torch.ones(1)         # 一个1维向量，值为1
#a = a.cuda()              # 将a放入GPU，如果本机没有GPU，注释此句
print(a.requires_grad)           # False
a.requires_grad = True    # 设定a需要计算导数
b = torch.ones(1)
x = 3 * a + b             # x是最终结果
print(x.requires_grad)    # True，因为a需要计算导数，所以x需要计算导数
x.backward()              # 计算所有参数的导数
print(a.grad)                   # tensor([ 3.])，导数为3 

print("=====全连接层示例=====")

import torch.nn as nn
# 四层神经网络，输入层大小为30，两个隐藏层大小为50和70，输出层大小为1
linear1 = nn.Linear(30, 50)
linear2 = nn.Linear(50, 70)
linear3 = nn.Linear(70, 1)
# 10组输入数据作为一批次(batch)，每一个输入为30维
x = torch.randn(10, 30)
# 10组输出数据，每一个输出为1维
res = linear3(linear2(linear1(x)))
print(res)

print("=====丢弃示例=====")

layer = nn.Dropout(0.1)  # Dropout层，置零概率为0.1
input = torch.randn(5, 2)
print(input)
output = layer(input)        # 维度仍为5 x 2，每个元素有10%概率为0
print(output)

print("=====CNN示例=====")

# 卷积神经网络，输入通道有1个，输出通道3个，过滤器大小为5
conv = nn.Conv2d(1, 3, 5)
# 10组输入数据作为一批次(batch)，每一个输入为单通道32 x 32矩阵
x = torch.randn(10, 1, 32, 32)  
# y维度为10 x 3 x 28 x 28，表示输出10组数据，每一个输出为3通道28 x 28矩阵 (28=32-5+1)
y = conv(x)
print(y.shape) # torch.Size([10, 3, 28, 28]

print("=====RNN示例=====")

# 双层GRU输入元素维度是10，状态维度是20，batch是第1维
rnn = nn.GRU(10, 20, num_layers=2)    
# 一批次共3个序列，每个序列长度5，维度是10，注意batch是第1维
x = torch.randn(5, 3, 10) 
# 初始状态，共3个序列，2层，维度是20
h0 = torch.randn(2, 3, 20)
# output是所有的RNN状态，大小为5 x 3 x 20；hn大小为2 x 3 x 20，为RNN最后一个状态
output, hn = rnn(x, h0) 
print(output.shape) # torch.Size([5, 3, 20])
print(hn.shape) # torch.Size([2, 3, 20])
