#深度学习框架PyTorch基础   P244

'''
机器学习基础 P205
按照训练数据中有无输出数据的标注分为监督学习(supervised learning)、无监督学习和半监督学习(semi-supervised learning)
监督学习的训练数据中包括输入范例和对应的输出范例 利用观察和统计输入输出之间的关系训练模型 并通过比对训练结果和标注结果之间的差异调整模型参数 提高预测性能 效果一般较好 但非常依赖标注数据的大小 亦花费大量人力和时间成本
无监督学习指训练数据没有对输出进行标注 需要模型自行定义训练标准 以达预期目标 如聚类 难度远大于监督学习 效果一般也逊于后者 可以有效利用海量未标注数据 特别适合NLP相关应用
半监督学习处理上述两者之间 一种方法是未标注数据进行预训练 通过少量标注数据引导模型进一步提高准确度

模型 建立模型是为了找到自变量房子的d种信息x=(x1,x2,...,xd)与因变量y价格的关系
参数 可能的关系有无数种 通过参数β0,β1...,βd来限定范围 y = β0 + β1x1 + β2x2 +...+ βdxd
     二次模型：y=β(0,0)+∑(i,∑(j,β(1,j)xixj))
不同模型的假设范围大小不一 参数个数和复杂程度也不一样 需要将参数调整到合适的值 训练模型的过程是根据输入和输出不断改变模型参数的取值 以得到具有一定准确度的模型 即参数优化(parameter optimization)

训练集 (training set)模型用于训练的数据
测试集 (test set)推广 泛化 在训练集上检验模型性能无法充分区分模型优劣 因此需要同分布却未出现的数据担当测试集 即关心模型在未见过数据上的推广能力 亦称泛化能力
过拟合 (overfitting) 模型本应在训练集上不断提升准确度 却在超过一定的阈值后于训练集上表现越好的模型在测试集上表现越差
验证集 (validation set) 为在训练中观察到过拟合现象 将训练集中分出一部分数据作为验证集 验证集准确度有下滑即可中止优化 选出之前验证集表现最好的模型作结果
'''


'''
深度学习基础 P208
深度学习 最重要的基础组成部分是神经网络
神经网络 指按照人类脑部神经元工作原理进行抽象而建立的一种人工智能运算模型 基本组成单元是人工神经元(neuron) 可输入处理及输出信息 由有权重的边相互连接并进行信息传递 大量神经元组成的复杂网络即神经网络
神经元 亦称感知元(perceptron) 神经网络的最小组成单元 包含n条输入信息x=(x1,x2,...,xn)和1条输出信息 信息均为实数 n条边的权重参数w=(w1,w2,...,wn)
     神经元进行加权和处理 U(x;w)=w1x1+w2x2+...+wnxn=∑(n,i+1,wixi)=w^Tx
     为输出更加灵活一般给结果加上截距 S(x;w,b)=U(x,w)+b=W^T+b
     即S(x;w,b)是关于输入(x1,x2,...,xn)的线性函数
     通过激活函数(activation function)g对加权和进行变换 得到最终输出f(x;w,b)=g(S(x;w,b))
激活函数 性质:1)是一个连续可导的一元非线性函数 允许在常数个点上不可导 目的是便于已有的数值优化工具对其进行求导并优化
             2)激活函数和其导数的计算须较为简单 以提高计算效率 因网络中大量参与运算和求导
             3)因神经网络求导中运用到乘积形式的链式法则 激活函数的导数值域需在一个合理的区间中 以防链式法则中导数的绝对值过大或过小
         常见形式:1)sigmoid函数 σ(x)=1/(1+e^-x) S形曲线 x=0时取值0.5 x→+∞时趋向1 x→-∞时趋向0 收敛速度很快 导数σ'(x)=σ(x)(1-σ(x)) 导数值域(0,0.25] 连续可导 运算简单 函数值域和导数值域均为限定区间 最常用的激活函数之一
                 2)tanh函数 双正切函数 tanh(x)=(e^x-e^-x)/(e^x+e^-x) S行曲线 值域(-1,1) x=0时取值0 x→+∞时趋向1 x→-∞时趋向-1 收敛速度很快 导数tanh'(x)=1-tanh^2(x) 导数值域(0,1] 原点对称 零中心化(zero-centered)
                 3)ReLU函数 修正线性单元(Rectified Linear Unit) 小于0时输出0 大于等于0时直接输出ReLU(x)={x,x>=0;0,x<0}=max(x,0) 导数远离零点 可使优化算法对参数进行有效调控 缺点当参数小于0导数永远是0优化算法无法改变参数的值

前馈神经网络 (feedforward neural network) n个神经元共享输入 并经过计算得到n个输出 形成一个层(level) 作为下一层神经元的输入 层与层通过有权重的边进行连接 形成基本的神经网络形式
     组成 第一层:输入层(Input Level) 非神经元 n个输入值 整个网络的外部输入
          最后一层:输出层(Output Level) 输出作为网络的输出
          两者间层:隐藏层(Hidden Level) 之间的层的统称
     所有相邻层之间均由带权边连接 每层神经元个数不必一致 有各自的截距和参数 充分灵活
     计算过程被称作向前传播(forward pass)
     
损失函数 (loss function)训练神经网络的目的在于使其输出符合预期真值(ground truth) 但其参数在初始化时是随机分布的 需要调整网络参数以提高模型表现 一种方法是通过输出和预期真值的差值求导来得到调整参数的方向 但该差值往往不是一个可导数如分类任务中的正确率 于是多由损失函数估算 损失函数是神经网络中所有参数的函数 它刻画了网络输出与预期真值之间的差异 对同一问题损失函数定义不唯一 但需三点性质：
     1.需要关于神经网络中的所有参数可导 2.定义是函数值越小模型准确度越高 3.需要近似表示结果的不准确性
          近似表示:真正的评测标准下对于一个输入网络参数产生结果好于另组网络参数及结果 无需保证损失函数结果最优 但需要大多情况下满足
     常见类型:均方差损失函数 均方差(Mean Squared Error,MSE) “差的平方的平方”的缩写 若预测值与预期真值接近 损失函数值以平方级缩小 适用于回归类(regression)问题
             交叉熵损失函数 如分类任务 取最大分数下标对于网络参数不可导 通过softmax操作将分数变为概率值 交叉熵希望最大化对应类的概率 即最大化可能性的对数 即最小化预测正确类别的概率的对数的相反数 因概率值范围0~1 交叉熵一定是非负数

梯度下降 (gradient descent)神经网络中最常用的优化方法 一种系统的优化方法来更新初始随机的参数 以达到损失函数值不断减小的目的
     梯度 (gradient)对于有多个参数的函数 可求出函数对于每一个参数的导数 这些导数形成的向量称为梯度
          如f(x)=f(x1,x2,...,xn) 每参数导数∂f/∂xi 导数形成的向量∇f(x)|x=(x1,x2,...,xn)=[∂f/∂x1,∂f/∂x2,...,∂f/∂xn]为梯度
     学习率 (learning rate)所有的参数沿着其梯度的反方向移动 距离用学习率α控制 即xi←xi-α(∂f/∂xi)
反向传播 (back propagation)梯度下降法为求出函数当前参数点梯度 即损失函数关于神经网络中所有参数的导数 神经网络结构复杂难以得到每个参数导数公式 遂采用反向传播 利用了微积分中导数计算的链式法则(chain rule)
     链式法则 如果f(g(x))是一个复合函数 即f是g的函数 而g是x的函数 那么f关于x的导数为df(g(x))/dx=(df(g)/dg)(dg(x)/dx)
       例如f(g)=g^2,g(x)=x^3 那么f(g(x))=x^6 所以df(g(x))/dx=6x^5 根据链式法则df(g(x))/dx=(df(g)/dg)(dg(x)/dx)=2g×3x^2=6x^5
       链式法则可以用在神经网络对参数求导的过程中 可得到损失函数关于最后一层边权的导数 继续计算关于前一层边权导数 直到得出全部导数 按学习率沿导数反向移动 以使损失值不断下降
     梯度爆炸 当网络层数变多 导数经过链式法则的乘法操作 很容易在靠近输入层的部分发生导数爆炸和导数消失的情况 多见于多个绝对值大于1的导数或多个绝对值小于1的导数相乘

深度学习中常见神经网络类型 P218
===========TODO:继续啃这里============
前馈神经网络 (FNN) 如上文 也被称为 全连接网络 (fully connected network) 相邻的两层间全部建立了连接每对神经元的边和权重 因每层神经元个数可能会很庞大 网络规模恐会平方级增长 造成存储空间巨大和运算速度低下 CNN为解决这个问题而设计 
卷积神经网络 (CNN) 最早应用于图像处理 像素的值和周围局部区域像素有关系 即具有局部性(locality) 如果把图像中每个像素最为一层中的一个神经元 那么它应该只和周围一部分神经元进行连接 输入层为I 隐藏层为I×K 每个隐藏层的神经元只和输入层的3×3=9个神经元连接 边权为K 如果依次高亮每个隐藏层神经元连接的输入层神经元 得到一个3×3的方块在输入层移动
    若输入层5×5=25个神经元 隐藏层9个神经元 全连接网络边权共有25×9=225个参数 可将所有隐藏层神经元的9个边权共享(weight sharing) 即统一使用K最为边权参数 便仅仅需要9个边权即可实现两层间的互联 这里称K为过滤器(filter) 这种网络即CNN 因为K和I中每个方块依次相乘并求和的操作类似于数学中的卷积运算
  卷积核 (Convolution Kernel，书中没有的概念) 图像处理时给定输入图像中一个小区域中像素加权平均后成为输出图像中的每个对应像素 其中权值由一个函数定义 这个函数称为卷积核 如卷积公式R(u,v)=∑∑G(u-i,v-j)f(i,j) 其中f为输入G为卷积核
     卷积核kernels：二维的矩阵；滤波器filters：多个卷积核组成的三维矩阵，多出的一维是通道。一个“Kernel”更倾向于是2D的权重矩阵。而“filter”则是指多个Kernel堆叠的3D结构。如果是一个2D的filter，那么两者就是一样的。但是一个3Dfilter，在大多数深度学习的卷积中，它是包含kernel的。每个卷积核都是独一无二的，主要在于强调输入通道的不同方面。(个人理解 类似滤波器 移动的3×3方块即卷积核 需要进一步熟悉)
  输出通道 (output channel) CNN中 也可以用两个不同的过滤器K1、K2生成I×K1和I×K2 称有两个输出通道 如果有多于1个的输入通道(input channel) 如图像中有红蓝绿3个通道 则可将每个输入通道和一份K卷积然后相加 由于输入有3个通道 每个过滤器有3层 与对应的输入进行卷积 得到3个3×3的矩阵 然后将这3个卷积结果进行同位置累加 得到有一个输出通道的3×3的矩阵 若输出通道有2个 则将2个过滤器的结果同时输出
  机器阅读理解中 卷积神经网络常用来进行字符编码处理 即每个长度为L的单词看做一副有1×L个像素的图 每个像素对应一个字符

循环神经网络 (RNN) 专门为深度学习处理变长数据设计的网络结构 对于输入中的每个元素 使用同样的网络结构S并共享参数 并在相邻元素之间进行信息和状态传递 好处在于既减少参数个数 又实现了对任意长度输入序列的处理 且通过信息的传递保留了序列的内在结构 RNN亦称为含上下文的词向量或上下文编码(contextualized embedding) 常用网络结构S:
  门控循环单元 (GRU)
  长短期记忆 (LSTM)

丢弃 (Dropout)
(没有提及对抗生成网络？)

深度学习框架PyTorch P224 如下
'''

print("=====初始化张量代码示例=====")

import torch
#一个大小为2 x 3的实数型张量
a = torch.FloatTensor([[1.2, 3.4, 5], [3, 6, 7.4]])
print(a)
#一个大小为5 x 6的实数型张量，每个元素根据标准正态分布N(0,1)随机采样
b = torch.randn(5, 6) 
print(b)
#将a第0行第2列的元素改为4.0，变为([[1.2, 3.4, 4.0], [3, 6, 7.4]])
a[0, 2] = 4.0 
print(a)

print("=====运算和求导示例=====")

import torch
a = torch.ones(1)         # 一个1维向量，值为1
#a = a.cuda()              # 将a放入GPU，如果本机没有GPU，注释此句
print(a.requires_grad)           # False
a.requires_grad = True    # 设定a需要计算导数
b = torch.ones(1)
x = 3 * a + b             # x是最终结果
print(x.requires_grad)    # True，因为a需要计算导数，所以x需要计算导数
x.backward()              # 计算所有参数的导数
print(a.grad)                   # tensor([ 3.])，导数为3 

print("=====全连接层示例=====")

import torch.nn as nn
# 四层神经网络，输入层大小为30，两个隐藏层大小为50和70，输出层大小为1
linear1 = nn.Linear(30, 50)
linear2 = nn.Linear(50, 70)
linear3 = nn.Linear(70, 1)
# 10组输入数据作为一批次(batch)，每一个输入为30维
x = torch.randn(10, 30)
# 10组输出数据，每一个输出为1维
res = linear3(linear2(linear1(x)))
print(res)

print("=====丢弃示例=====")

layer = nn.Dropout(0.1)  # Dropout层，置零概率为0.1
input = torch.randn(5, 2)
print(input)
output = layer(input)        # 维度仍为5 x 2，每个元素有10%概率为0
print(output)

print("=====CNN示例=====")

# 卷积神经网络，输入通道有1个，输出通道3个，过滤器大小为5
conv = nn.Conv2d(1, 3, 5)
# 10组输入数据作为一批次(batch)，每一个输入为单通道32 x 32矩阵
x = torch.randn(10, 1, 32, 32)  
# y维度为10 x 3 x 28 x 28，表示输出10组数据，每一个输出为3通道28 x 28矩阵 (28=32-5+1)
y = conv(x)
print(y.shape) # torch.Size([10, 3, 28, 28]

print("=====RNN示例=====")

# 双层GRU输入元素维度是10，状态维度是20，batch是第1维
rnn = nn.GRU(10, 20, num_layers=2)    
# 一批次共3个序列，每个序列长度5，维度是10，注意batch是第1维
x = torch.randn(5, 3, 10) 
# 初始状态，共3个序列，2层，维度是20
h0 = torch.randn(2, 3, 20)
# output是所有的RNN状态，大小为5 x 3 x 20；hn大小为2 x 3 x 20，为RNN最后一个状态
output, hn = rnn(x, h0) 
print(output.shape) # torch.Size([5, 3, 20])
print(hn.shape) # torch.Size([2, 3, 20])
